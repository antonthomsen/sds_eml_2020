{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 1: Lending club\n",
    "\n",
    "*February 5, 2019*\n",
    "\n",
    "In this Exercise Set 1 we will investigate loan data from `Lending Club`. We will see how nested cross validation can be used  to compute a distribution of performance metrics for comparing different models. We will also try out bagging.\n",
    "\n",
    "Below we provide some code that helps by downloading and cleaning the data. Note this takes around 1 min if you have fast internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: there are three .zip files with letter = a,b,c. \n",
    "# To ensure the files download in reasonable time we \n",
    "# only work with the first of the three. If you have time\n",
    "# you can modify this cell to download all three. \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "filenames = []\n",
    "base_url = 'https://resources.lendingclub.com/'\n",
    "\n",
    "letter = 'a'\n",
    "filename = f'LoanStats3{letter}.csv.zip'\n",
    "url = base_url+filename\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)\n",
    "filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target we are going to predict is whether loan is repaid. This can be extracted from the `loan_status` below. Features names that we work with:\n",
    "\n",
    "- **annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
    "- **dti**: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
    "- **emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. \n",
    "- **grade**: LC assigned loan grade\n",
    "- **home_ownership**: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
    "- **int_rate**: Interest Rate on the loan\n",
    "- **term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "- **verification_status**: Indicates if income was verified by LC, not verified, or if the income source was verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in csv files, store them\n",
    "dfs = [pd.read_csv(f,header=0,skiprows=1,low_memory=False) for f in filenames]\n",
    "\n",
    "# concatenate the dataframes (as standard there is only 1)\n",
    "df = pd.concat(dfs)\\\n",
    "        .dropna(subset=['loan_amnt'])\\\n",
    "        .dropna(axis=1, how='all')\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify loans of interest\n",
    "df = df.loc[df.loan_status.isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "\n",
    "# Clean up variables \n",
    "df['charged_off'] = (df.loan_status=='Charged Off').astype(int)\n",
    "df['int_rate_f'] = df.int_rate.str[:-1].astype(float)\n",
    "df['emp_length_f'] = df.emp_length\\\n",
    "                        .str.split(' ')\\\n",
    "                        .str[0].str[:2]\\\n",
    "                        .str.replace('<','0')\\\n",
    "                        .astype(float)\n",
    "\n",
    "# label and features\n",
    "y_var = 'charged_off'\n",
    "X_vars = ['term', 'int_rate_f', 'grade', 'home_ownership', 'emp_length_f',\n",
    "          'annual_inc', 'verification_status', 'dti']\n",
    "\n",
    "# Create dummies\n",
    "data = pd.get_dummies(df[X_vars+[y_var]], drop_first=True)\\\n",
    "        .dropna()\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .astype(np.float64)\\\n",
    "        .loc[:2000]\\\n",
    "        .copy()\n",
    "\n",
    "# View data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us split the data into test and training data. To do this we use the `StratifiedShuffleSplit` from scikit learn (read more about splitting methods [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=.3, random_state=3)\n",
    "\n",
    "# These are the row indices of the stratified split\n",
    "data_splits = list(sss.split(data[y_var], data[y_var]))\n",
    "\n",
    "# Separate data in y,X\n",
    "y = data[y_var]\n",
    "X_vars_b = data.columns!=y_var\n",
    "X = data.loc[:,X_vars_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.1 Model validation\n",
    "\n",
    "We start out with some basic concepts and our goal is to estimate and evaluate the logistic regression.\n",
    "\n",
    "\n",
    "> **Ex. 1.1.1:** extract the first data split and construct features, target for train, test data.\n",
    ">> *Hint:* `data_splits` is a list with 10 train-test pairs of randomly selected row indices. So the k'th element of `data_splits` contains a pair (train_idx, test_idx) identifying the k'th fold in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.1 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = data_splits[0]\n",
    "\n",
    "y_train = y.loc[train_idx]\n",
    "X_train = X.loc[train_idx]\n",
    "\n",
    "y_test = y.loc[test_idx]\n",
    "X_test = X.loc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.2:** estimate a logistic regression model on your training data. Then compute the overall accuracy and $F_1$ score for the default class *on the training data*.\n",
    ">\n",
    ">> **Note**: the code below implements a pipeline which standardizes the data by converting it into zero mean and unit standard deviation. We let `C=10**10` which corresponds to no regularization. Make sure you understand why each of these steps are important by looking up the scikit learn docs.\n",
    ">\n",
    ">> *Hint 2:* `sklearn` has these functions built-in and are named `f1_score`, `accuracy_score`. See Raschka pp. 189-193."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler        # scales variables to be mean=0,sd=1\n",
    "from sklearn.linear_model import LogisticRegression     # regression model\n",
    "from sklearn.pipeline import Pipeline                   # For building our model pipeline\n",
    "\n",
    "lr = Pipeline([('scale', StandardScaler()),\n",
    "               ('clf', LogisticRegression(class_weight='balanced',C=10**10, solver = 'liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.2 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_p_tr = lr.predict(X_train)\n",
    "acc_tr, f1_tr = accuracy_score(y_p_tr,y_train), f1_score(y_p_tr,y_train)\n",
    "acc_tr, f1_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.3:** Explain some advantages of computing the $F_1$ score vs. computing the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.3 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** $F_1$ is the harmonic mean of precision and recall (read the wiki for more info). With unbalanced data accuracy has the undesired property of being high under a decision rule that always assigns the majority class. Consider e.g. a dataset with 100 observations, of which 5 are \"good fish\" and 95 \"bad fish\"; always predicting \"bad fish\" results in a 95% accuracy. Note that this is not the unique way to get 95% accuracy, we could also imagine a model that correctly identifies all 5 good fish, but mislabels 5 bad fish. To solve this problem we want a model-performance-metric that is not blind to class imbalance.\n",
    "\n",
    "This is what the F1 score helps us with, it weights precision (how precisely our \"good fish\" label is being put on good fish) and recall (how many of total good fish we actually label \"good fish\"). You can read more on the F1 wiki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.4:** Compute the *test set* accuracy and $F_1$ score. How does these compare to the results you got in exercise 1.1.3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.4 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_ts = lr.predict(X_test)\n",
    "acc_tst, f1_tst = accuracy_score(y_p_ts,y_test), f1_score(y_p_ts,y_test)\n",
    "acc_tst, f1_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.5:** Explain why test set performance is often preferred to the training set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.5 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to avoid overfitting - the best way to do this is to hold out data to compare our estimates to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cross validation\n",
    "\n",
    "We now turn to actually optimizing the model. We will use cross validation to perform the optimization.\n",
    "\n",
    "> **Ex. 1.1.6:** Explain what the parameter `C` does in logistic regression. How do parameters change when `C` get smaller? Also explain what `penalty='l1'` will do for coefficients.\n",
    ">> *Hint:* The documentation for scikit learn will tell you everything you need to know about their implementation of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.6 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- `C` corresponds to $\\lambda^{-1}$ in the standard formulation of where $\\beta=\\arg\\min_{\\beta}MSE(\\beta)+ \\lambda\\sum |\\beta|^L$\n",
    "- in the above `l2` regularization is when $L=2$; this leads to smaller parameters\n",
    "- in the above `l1` regularization is when $L=1$; this leads to removal of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.7:** Use cross validation on training set to estimate hyperparameters and then re-estimate model on training set. The set of $\\lambda$ to be considered are $\\{10^{-4},10^{-2},1,10^{2},10^{4}\\}$. Is the model with optimized hyperparameters better than the non-optimized?\n",
    ">\n",
    ">> *Hint 1:* This procedure is implemented in `GridSearchCV`. You set `n_jobs=-1` to parallize computation. See Raschka pp. 186-187 for inspiration.\n",
    ">\n",
    ">> *Hint 2:* Consider using `np.logspace` for making the set of $\\lambda$.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_cv = GridSearchCV(estimator=lr,\n",
    "                     param_grid={'clf__C':np.logspace(-4,4,5)},\n",
    "                     n_jobs=-1,\n",
    "                     cv=3)\n",
    "lr_cv.fit(X_train, y_train)\n",
    "\n",
    "y_p_tst_cv = lr_cv.predict(X_test)\n",
    "acc_tst_cv = accuracy_score(y_p_tst_cv, y_test)\n",
    "f1_tst_cv = f1_score(y_p_tst_cv, y_test)\n",
    "acc_tst_cv, f1_tst_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.8:** Apply nested resampling to compute a distribution of test scores with and without optimization. You should use `data_splits` which we defined initially and input all the data.\n",
    ">\n",
    ">> *Hint:* You can implement this using your code from Ex. 1.1.6 and combine it with `cross_val_score`. Note that `cv` input should use `data_splits`. See Raschka pp. 188-189 for inspiration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.8 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem will be in assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.9:** Plot the distributions of the optimized vs. the non-optimized. What do you conclude about the performance difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.9 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-84260a69cbe4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m cv_res = pd.DataFrame({'Not optimized':nested_cv_score_nonopt, \n\u001b[0m\u001b[0;32m      4\u001b[0m                        'Optimized':nested_cv_score_opt})\n\u001b[0;32m      5\u001b[0m \u001b[0mcv_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "cv_res = pd.DataFrame({'Not optimized':nested_cv_score_nonopt, \n",
    "                       'Optimized':nested_cv_score_opt})\n",
    "cv_res.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.10** Argue how $F_1$ differs from  *area under the curve* (AUC). Comment on their theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- $F_1$ is a concrete measure based on actual predictions. \n",
    "- AUC is a theoretical measure which uses the predicted probabilities. For each possible threshold for predicting Positive we need to compute the  False Positive Rate (FPR) and True Positive Rate (TPR). AUC is the integral of the function that FPR maps into TPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification trees and forests\n",
    "\n",
    ">  **Ex. 1.1.11** Estimate a classification tree on the training data (with default hyperparameters). Evalate both on training and test data by computing the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You can check out code for Ex. 1.1.10 for inspiration. You may also want to look up `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a mistake in this problem. You cannot compute ROC curves \n",
    "# for a single decision tree, since they dont support .predict_proba. \n",
    "# Below is a solution using a random forest instead. \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "roc_auc_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.12** Estimate 50 classification trees and compute the mean prediction. Use the mean prediction to compute the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You may use the code block below and apply it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_tree(data_train):\n",
    "    '''\n",
    "    Trains a decision tree on a bootstrap \n",
    "    of training data.\n",
    "    '''\n",
    "    df_ = data_train.sample(frac=1, replace=True)\n",
    "    ct = DecisionTreeClassifier(class_weight='balanced')\n",
    "    ct.fit(df_.loc[:,X_vars_b], df_[y_var])\n",
    "    return ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.12 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1a81bfbc8813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Fit 50 decision trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Train data\n",
    "data_train =  data.loc[train_idx,:]\n",
    "\n",
    "# Fit 50 decision trees\n",
    "CTs = [train_tree(data_train) for i in range(50)]\n",
    "\n",
    "# For both train and test do:\n",
    "for label, idxs in [('train', train_idx), ('test', test_idx)]:\n",
    "    \n",
    "    # 0s with shape n-samples x n-models\n",
    "    y_p_CT_ = np.zeros((idxs.shape[0], len(CTs)))\n",
    "    \n",
    "    # For each model index\n",
    "    for i in range(len(CTs)):\n",
    "        # Store models predicted values in the 0-matrix\n",
    "        y_p_CT_[:,i] = CTs[i].predict(X.loc[idxs])\n",
    "    # print average AUC\n",
    "    print(roc_auc_score(y_p_CT_.mean(1)>.2, y.loc[idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.13** Is Random Forest classification different from the procedure of aggregating tree predictions above? If so, explain how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.13 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes they are different. RF also bootstraps which features to use; this is called feature bagging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
